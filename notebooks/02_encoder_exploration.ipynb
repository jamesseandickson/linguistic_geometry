{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a907fc1",
   "metadata": {},
   "source": [
    "# Encoder Exploration\n",
    "\n",
    "This notebook explores different text encoders for the Linguistic Geometry project.\n",
    "\n",
    "**What we'll test:**\n",
    "1. Basic encoding functionality\n",
    "2. Semantic similarity patterns\n",
    "3. Corpus encoding performance\n",
    "4. Model comparisons\n",
    "\n",
    "**Goal:** Understand how different encoders represent semantic concepts as numerical embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586a1bf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdfb2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from encoders import SentenceTransformerEncoder\n",
    "from corpora.loader import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa052c7",
   "metadata": {},
   "source": [
    "## Test 1: Basic Encoding Functionality\n",
    "\n",
    "Let's start by testing that our encoder can convert text to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e280e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing Basic Encoding\n",
      "================================================================================\n",
      "\n",
      "Loading encoder (all-MiniLM-L6-v2)...\n",
      "âœ“ Encoder loaded: SentenceTransformerEncoder(model=all-MiniLM-L6-v2)\n",
      "  Embedding dimension: 384D\n",
      "\n",
      "Single text: 'hello world'\n",
      "  Embedding shape: (1, 384)\n",
      "  First 5 values: [-0.03447728  0.03102317  0.00673496  0.02610902 -0.03936203]\n",
      "\n",
      "Multiple texts: ['happy', 'sad', 'joyful', 'miserable']\n",
      "  Embeddings shape: (4, 384)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing Basic Encoding\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Initialize encoder\n",
    "print(\"Loading encoder (all-MiniLM-L6-v2)...\")\n",
    "encoder = SentenceTransformerEncoder()\n",
    "print(f\"âœ“ Encoder loaded: {encoder}\")\n",
    "print(f\"  Embedding dimension: {encoder.embedding_dim}D\")\n",
    "print()\n",
    "\n",
    "# Test single string\n",
    "text = \"hello world\"\n",
    "embedding = encoder.encode(text)\n",
    "print(f\"Single text: '{text}'\")\n",
    "print(f\"  Embedding shape: {embedding.shape}\")\n",
    "print(f\"  First 5 values: {embedding[0, :5]}\")\n",
    "print()\n",
    "\n",
    "# Test multiple strings\n",
    "texts = [\"happy\", \"sad\", \"joyful\", \"miserable\"]\n",
    "embeddings = encoder.encode(texts)\n",
    "print(f\"Multiple texts: {texts}\")\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea81c44",
   "metadata": {},
   "source": [
    "## Test 2: Semantic Similarity\n",
    "\n",
    "Let's test that semantically similar words have similar embeddings.\n",
    "We'll compute cosine similarities between word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bc9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing Semantic Similarity\n",
      "================================================================================\n",
      "\n",
      "Cosine similarities:\n",
      "            happy     joyful    sad       freezing  hot       \n",
      "happy            1.000     0.684     0.373     0.190     0.464\n",
      "joyful           0.684     1.000     0.269     0.260     0.363\n",
      "sad              0.373     0.269     1.000     0.177     0.268\n",
      "freezing         0.190     0.260     0.177     1.000     0.300\n",
      "hot              0.464     0.363     0.268     0.300     1.000\n",
      "\n",
      "Expected patterns:\n",
      "  happy â†” joyful:   0.684 (should be high)\n",
      "  happy â†” sad:      0.373 (should be medium)\n",
      "  happy â†” freezing: 0.190 (should be low)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing Semantic Similarity\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "encoder = SentenceTransformerEncoder()\n",
    "\n",
    "# Encode words\n",
    "words = [\"happy\", \"joyful\", \"sad\", \"freezing\", \"hot\"]\n",
    "embeddings = encoder.encode(words)\n",
    "\n",
    "# Compute cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "print(f\"{'':12s}\", end=\"\")\n",
    "for word in words:\n",
    "    print(f\"{word:10s}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word:12s}\", end=\"\")\n",
    "    for j in range(len(words)):\n",
    "        print(f\"{similarities[i, j]:10.3f}\", end=\"\")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "# Check specific pairs\n",
    "happy_joyful = similarities[0, 1]\n",
    "happy_sad = similarities[0, 2]\n",
    "happy_freezing = similarities[0, 3]\n",
    "\n",
    "print(\"Expected patterns:\")\n",
    "print(f\"  happy â†” joyful:   {happy_joyful:.3f} (should be high)\")\n",
    "print(f\"  happy â†” sad:      {happy_sad:.3f} (should be medium)\")\n",
    "print(f\"  happy â†” freezing: {happy_freezing:.3f} (should be low)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569910c",
   "metadata": {},
   "source": [
    "## Test 3: Corpus Encoding\n",
    "\n",
    "Now let's test encoding a concept cluster from our semantic corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6eaedaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing Corpus Encoding\n",
      "================================================================================\n",
      "\n",
      "Loaded corpus: semantic_concepts_v0\n",
      "Total clusters: 20\n",
      "\n",
      "Test cluster: emotion/valence_mixed\n",
      "Concept: affective_valence_mixed\n",
      "Size: 20 expressions\n",
      "Examples: happy, joyful, content, pleased, cheerful\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a0c1f0a8a4f3eb04984e24f43049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Encoded 20 expressions\n",
      "  Embeddings shape: (20, 384)\n",
      "  Memory: 30.0 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Testing Corpus Encoding\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load corpus\n",
    "corpus = load_corpus(\"semantic_concepts_v0\")\n",
    "print(f\"Loaded corpus: {corpus.corpus_id}\")\n",
    "print(f\"Total clusters: {len(corpus)}\")\n",
    "print()\n",
    "\n",
    "# Get first cluster\n",
    "cluster = corpus.clusters[0]\n",
    "print(f\"Test cluster: {cluster.domain}/{cluster.subdomain}\")\n",
    "print(f\"Concept: {cluster.concept}\")\n",
    "print(f\"Size: {len(cluster)} expressions\")\n",
    "print(f\"Examples: {', '.join(cluster.expressions[:5])}\")\n",
    "print()\n",
    "\n",
    "# Encode cluster\n",
    "encoder = SentenceTransformerEncoder()\n",
    "embeddings = encoder.encode(cluster.expressions, show_progress_bar=True)\n",
    "\n",
    "print(f\"âœ“ Encoded {len(cluster)} expressions\")\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  Memory: {embeddings.nbytes / 1024:.1f} KB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f9b62",
   "metadata": {},
   "source": [
    "## Test 4: Model Comparison\n",
    "\n",
    "Let's compare different sentence-transformer models to see their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec955e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Comparing Different Models\n",
      "================================================================================\n",
      "\n",
      "Model: all-MiniLM-L6-v2\n",
      "  Description: Fast & lightweight\n",
      "  âœ“ Dimension: 384D\n",
      "  âœ“ Shape: (1, 384)\n",
      "\n",
      "Model: all-mpnet-base-v2\n",
      "  Description: High quality\n",
      "  âœ“ Dimension: 768D\n",
      "  âœ“ Shape: (1, 768)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Comparing Different Models\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "models = [\n",
    "    (\"all-MiniLM-L6-v2\", \"Fast & lightweight\"),\n",
    "    (\"all-mpnet-base-v2\", \"High quality\"),\n",
    "]\n",
    "\n",
    "test_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "for model_name, description in models:\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  Description: {description}\")\n",
    "    \n",
    "    try:\n",
    "        encoder = SentenceTransformerEncoder(model_name)\n",
    "        embedding = encoder.encode(test_text)\n",
    "        \n",
    "        print(f\"  âœ“ Dimension: {encoder.embedding_dim}D\")\n",
    "        print(f\"  âœ“ Shape: {embedding.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db9266",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic Encoding** - Converting text to numerical embeddings\n",
    "2. **Semantic Similarity** - Similar words have similar embeddings\n",
    "3. **Corpus Integration** - Encoding concept clusters from our semantic corpus\n",
    "4. **Model Comparison** - Different models have different characteristics\n",
    "\n",
    "**Next Steps:**\n",
    "- Connect these encoders to the geometry fitting system\n",
    "- Test which geometric structures best explain semantic concepts\n",
    "- Discover if emotions, temperatures, etc. have intrinsic geometric patterns\n",
    "\n",
    "**Key Insight:** We now have a working system to convert semantic concepts into numerical representations that can be analyzed for geometric structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ef618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Encoder exploration complete!\n",
      "Ready to discover geometric patterns in language!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ‰ Encoder exploration complete!\")\n",
    "print(\"Ready to discover geometric patterns in language!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
